{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 589 Artificial Neural Networks Project 2\n",
    "## Handwritten Character Recognition\n",
    "\n",
    "This program uses as an Artifical Neural Network with one hidden layer to predict the identity of handwritten characters of all 26 letters of the english alphabet.\n",
    "\n",
    "Dataset: EMNIST Letters - contains 145,600 28x28px images of handwritten characters divided into 26 classes. The dataset is divided into 108,000 training samples, 20,800 validation samples, and 20,800 test samples. More information about the dataset can be found here: https://www.nist.gov/itl/products-and-services/emnist-dataset\n",
    "\n",
    "This project utilizes the tf.keras module of TensorFlow to create, train, and test the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import dirname, join as pjoin\n",
    "from scipy import io as sio\n",
    "#from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation\n",
    "\n",
    "In this section we access the emnist data set using sio.loadmat(). The 28x28px image has been flattened into a 784 vector.\n",
    "\n",
    "We then divide the dataset into training, validation, and test sets. The emnist data is already divided into training and test sets. However, we obtain the validation set from taking the 5% off the end of the training data (so that it is the same size as the test set). \n",
    "\n",
    "Finally, we convert each of the label vectors to categorical arrays that can be recognized by the keras model we will be creating. Therefore, for each 28x28px image (or 784 1D vector) there is exactly one vector of length 26 to label which letter of the alphabet is represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104000 training samples\n",
      "20800 validation samples\n",
      "20800 test samples\n",
      "(20800, 784) (20800, 26)\n"
     ]
    }
   ],
   "source": [
    "# Define some vars we will use to construct the model\n",
    "num_classes = 26\n",
    "batch_size = 3000\n",
    "epochs = 25\n",
    "neurons = 1000\n",
    "\n",
    "mat_contents = sio.loadmat('emnist-letters.mat')\n",
    "data = mat_contents['dataset']\n",
    "\n",
    "X_train = data['train'][0,0]['images'][0,0]\n",
    "y_train = data['train'][0,0]['labels'][0,0]\n",
    "X_test = data['test'][0,0]['images'][0,0]\n",
    "y_test = data['test'][0,0]['labels'][0,0]\n",
    "\n",
    "val_start = X_train.shape[0] - X_test.shape[0]\n",
    "X_val = X_train[val_start:X_train.shape[0],:]\n",
    "y_val = y_train[val_start:X_train.shape[0]]\n",
    "X_train = X_train[0:val_start,:]\n",
    "y_train = y_train[0:val_start]\n",
    "\n",
    "y_train -=1\n",
    "y_val -=1\n",
    "y_test -=1\n",
    "\n",
    "#convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(X_train.shape[0], \"training samples\")\n",
    "print(X_val.shape[0], \"validation samples\")\n",
    "print(X_test.shape[0], \"test samples\")\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "Here, we create the dense neural network layer by layer, using tr.keras.\n",
    "\n",
    "The model is first initialized with a 784 neuron inpupt layer. \n",
    "\n",
    "The amount of neurons in the hidden layer was somethings that we played around with a lot. See the table below for all hyperparameters and how they affected accuracy. We eventually settled on a hidden layer with 1000 neurons to maximize efficiency and accuracy.\n",
    "\n",
    "The final layer consists of 26 neurons (one for each letter) and softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 200)               200200    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 26)                5226      \n",
      "=================================================================\n",
      "Total params: 1,991,426\n",
      "Trainable params: 1,991,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(1000, activation='relu', input_shape=(784,)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(1000, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compilation and Training\n",
    "\n",
    "Here the model is compiled and trained with the parameters that we finalized.\n",
    "\n",
    "This table shows some insight into the different hyperparameters we tested, and the impact they had on model accuracy:\n",
    "\n",
    "| Epochs | Batch Size | Hidden Layer Neurons | Accuracy |\n",
    "|--------|------------|----------------------|----------|\n",
    "|20      |1000        |800                   |89.69%    |\n",
    "|20      |2000        |800                   |89.56%    |\n",
    "|20      |1000        |500                   |88.58%    |\n",
    "|20      |1000        |1000                  |89.85%    |\n",
    "|20      |2000        |1000                  |90.25%    |\n",
    "|20      |2000        |800                   |89.74%    |\n",
    "|20      |2000        |1200                  |90.27%    |\n",
    "|20      |2500        |1200                  |89.64%    |\n",
    "|15      |2000        |1200                  |89.67%    |\n",
    "|20      |1500        |1200                  |89.56%    |\n",
    "|20      |1500        |800                   |89.57%    |\n",
    "|20      |1000        |800                   |89.18%    |\n",
    "|20      |2000        |800                   |89.81%    |\n",
    "|20      |2000        |1200                  |89.62%    |\n",
    "|20      |2000        |1000                  |90.02%    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 104000 samples, validate on 20800 samples\n",
      "Epoch 1/25\n",
      "104000/104000 [==============================] - 5s 50us/sample - loss: 38.0192 - accuracy: 0.1564 - val_loss: 2.4657 - val_accuracy: 0.3198\n",
      "Epoch 2/25\n",
      "104000/104000 [==============================] - 5s 47us/sample - loss: 2.2389 - accuracy: 0.3905 - val_loss: 1.3583 - val_accuracy: 0.6341\n",
      "Epoch 3/25\n",
      "104000/104000 [==============================] - 5s 50us/sample - loss: 1.5007 - accuracy: 0.5901 - val_loss: 0.9611 - val_accuracy: 0.7263\n",
      "Epoch 4/25\n",
      "104000/104000 [==============================] - 5s 51us/sample - loss: 1.1023 - accuracy: 0.6924 - val_loss: 0.6821 - val_accuracy: 0.8022\n",
      "Epoch 5/25\n",
      "104000/104000 [==============================] - 5s 52us/sample - loss: 0.8635 - accuracy: 0.7532 - val_loss: 0.7549 - val_accuracy: 0.7752\n",
      "Epoch 6/25\n",
      "104000/104000 [==============================] - 6s 57us/sample - loss: 0.7205 - accuracy: 0.7899 - val_loss: 0.5701 - val_accuracy: 0.8333\n",
      "Epoch 7/25\n",
      "104000/104000 [==============================] - 6s 60us/sample - loss: 0.6110 - accuracy: 0.8196 - val_loss: 0.4925 - val_accuracy: 0.8492\n",
      "Epoch 8/25\n",
      "104000/104000 [==============================] - 6s 59us/sample - loss: 0.5409 - accuracy: 0.8376 - val_loss: 0.4639 - val_accuracy: 0.8655\n",
      "Epoch 9/25\n",
      "104000/104000 [==============================] - 7s 63us/sample - loss: 0.4891 - accuracy: 0.8512 - val_loss: 0.4017 - val_accuracy: 0.8827\n",
      "Epoch 10/25\n",
      "104000/104000 [==============================] - 6s 59us/sample - loss: 0.4375 - accuracy: 0.8650 - val_loss: 0.4087 - val_accuracy: 0.8811\n",
      "Epoch 11/25\n",
      "104000/104000 [==============================] - 6s 60us/sample - loss: 0.4130 - accuracy: 0.8711 - val_loss: 0.4129 - val_accuracy: 0.8860\n",
      "Epoch 12/25\n",
      "104000/104000 [==============================] - 5s 52us/sample - loss: 0.3855 - accuracy: 0.8790 - val_loss: 0.3964 - val_accuracy: 0.8888\n",
      "Epoch 13/25\n",
      "104000/104000 [==============================] - 6s 56us/sample - loss: 0.3569 - accuracy: 0.8870 - val_loss: 0.3954 - val_accuracy: 0.8917\n",
      "Epoch 14/25\n",
      "104000/104000 [==============================] - 6s 57us/sample - loss: 0.3436 - accuracy: 0.8912 - val_loss: 0.3895 - val_accuracy: 0.8914\n",
      "Epoch 15/25\n",
      "104000/104000 [==============================] - 6s 54us/sample - loss: 0.3253 - accuracy: 0.8957 - val_loss: 0.3611 - val_accuracy: 0.8975\n",
      "Epoch 16/25\n",
      "104000/104000 [==============================] - 5s 53us/sample - loss: 0.3062 - accuracy: 0.9013 - val_loss: 0.3684 - val_accuracy: 0.8993\n",
      "Epoch 17/25\n",
      "104000/104000 [==============================] - 5s 53us/sample - loss: 0.2931 - accuracy: 0.9039 - val_loss: 0.3792 - val_accuracy: 0.8981\n",
      "Epoch 18/25\n",
      "104000/104000 [==============================] - 6s 54us/sample - loss: 0.2786 - accuracy: 0.9079 - val_loss: 0.3558 - val_accuracy: 0.9038\n",
      "Epoch 19/25\n",
      "104000/104000 [==============================] - 6s 53us/sample - loss: 0.2686 - accuracy: 0.9107 - val_loss: 0.3584 - val_accuracy: 0.9069\n",
      "Epoch 20/25\n",
      "104000/104000 [==============================] - 6s 53us/sample - loss: 0.2608 - accuracy: 0.9137 - val_loss: 0.3650 - val_accuracy: 0.9001\n",
      "Epoch 21/25\n",
      "104000/104000 [==============================] - 6s 54us/sample - loss: 0.2509 - accuracy: 0.9169 - val_loss: 0.3656 - val_accuracy: 0.9069\n",
      "Epoch 22/25\n",
      "104000/104000 [==============================] - 6s 55us/sample - loss: 0.2478 - accuracy: 0.9181 - val_loss: 0.3603 - val_accuracy: 0.9045\n",
      "Epoch 23/25\n",
      "104000/104000 [==============================] - 6s 54us/sample - loss: 0.2402 - accuracy: 0.9203 - val_loss: 0.3755 - val_accuracy: 0.9080\n",
      "Epoch 24/25\n",
      "104000/104000 [==============================] - 6s 55us/sample - loss: 0.2338 - accuracy: 0.9219 - val_loss: 0.3654 - val_accuracy: 0.9075\n",
      "Epoch 25/25\n",
      "104000/104000 [==============================] - 6s 54us/sample - loss: 0.2233 - accuracy: 0.9242 - val_loss: 0.3675 - val_accuracy: 0.9062\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = epochs,\n",
    "                    verbose = 1,\n",
    "                    validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "The model is evaluated by using the test set and the tf.keras() module. We output the final loss and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3719494960714935\n",
      "Test accuracy: 0.9057211\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "\n",
    "After trying many different hyperparameter values we eventually settled on the ones that gave us the highest prediction accuracy. We came to our final values by testing out a different number of epochs, batch size, and hidden neurons to reach an accuracy of 90.02%. Our final values were as follows:\n",
    "\n",
    "Epochs: 20\n",
    "\n",
    "Batch Size: 2000\n",
    "\n",
    "Hidden Neurons: 1000\n",
    "\n",
    "Some of the challenges that arose during this project were the manipulation of the dataset and creating the model so that it would run properly. For instance, a problem arose where we couldn't get the model to work with 26 classes because we had divided the training and validation data wrong. It was because of this that the model wouldn't run witht the proper number of classes. We had to do some research into the EMNIST data set and how it was formatted to finally get the model working properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
